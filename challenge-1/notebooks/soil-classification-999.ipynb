{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":102672,"databundleVersionId":12375409,"sourceType":"competition"}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport pandas as pd\n\nextraction_path = '/kaggle/input/soil-classification'\n\n# List the files and directories within the extraction_path\nprint(\"Contents of extraction_path:\")\nprint(os.listdir(extraction_path))\n\n# Inspect the contents of the train and test directories\ntrain_path = os.path.join(extraction_path, 'soil_classification-2025', 'train')\ntest_path = os.path.join(extraction_path, 'soil_classification-2025', 'test')\n\nprint(\"\\nContents of train directory:\")\nprint(os.listdir(train_path)[:10]) # Print only first 10 for brevity\n\nprint(\"\\nContents of test directory:\")\nprint(os.listdir(test_path)[:10]) # Print only first 10 for brevity\n\n# Read train_labels.csv\ntrain_labels_path = os.path.join(extraction_path, 'soil_classification-2025', 'train_labels.csv')\ntrain_labels_df = pd.read_csv(train_labels_path)\nprint(\"\\nHead of train_labels.csv:\")\ndisplay(train_labels_df.head())\n\n# Read test_ids.csv\ntest_ids_path = os.path.join(extraction_path, 'soil_classification-2025', 'test_ids.csv')\ntest_ids_df = pd.read_csv(test_ids_path)\nprint(\"\\nHead of test_ids.csv:\")\ndisplay(test_ids_df.head())","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-24T16:00:16.610255Z","iopub.execute_input":"2025-05-24T16:00:16.610457Z","iopub.status.idle":"2025-05-24T16:00:16.959890Z","shell.execute_reply.started":"2025-05-24T16:00:16.610439Z","shell.execute_reply":"2025-05-24T16:00:16.959222Z"}},"outputs":[{"name":"stdout","text":"Contents of extraction_path:\n['soil_classification-2025']\n\nContents of train directory:\n['img_3c4ed833.jpeg', 'img_8163dc71.jpg', 'img_e48dfef4.jpg', 'img_83d433d2.jpg', 'img_4b62f891.jpeg', 'img_15c3ff99.jpg', 'img_dacfe3f9.jpg', 'img_febe3434.jpeg', 'img_dbceabbb.jpg', 'img_e9f1d910.jpeg']\n\nContents of test directory:\n['img_0f035b97.jpg', 'img_f13af256.jpg', 'img_15b41dbc.jpg', 'img_cfb4fc7a.jpg', 'img_683111fb.jpg', 'img_c4bd7b3e.jpg', 'img_4ccce0f8.jpg', 'img_86faa98d.jpg', 'img_c448342c.jpg', 'img_e7f7c796.jpg']\n\nHead of train_labels.csv:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"           image_id      soil_type\n0  img_ed005410.jpg  Alluvial soil\n1  img_0c5ecd2a.jpg  Alluvial soil\n2  img_ed713bb5.jpg  Alluvial soil\n3  img_12c58874.jpg  Alluvial soil\n4  img_eff357af.jpg  Alluvial soil","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_id</th>\n      <th>soil_type</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>img_ed005410.jpg</td>\n      <td>Alluvial soil</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>img_0c5ecd2a.jpg</td>\n      <td>Alluvial soil</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>img_ed713bb5.jpg</td>\n      <td>Alluvial soil</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>img_12c58874.jpg</td>\n      <td>Alluvial soil</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>img_eff357af.jpg</td>\n      <td>Alluvial soil</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"name":"stdout","text":"\nHead of test_ids.csv:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"            image_id\n0  img_cdf80d6f.jpeg\n1   img_c0142a80.jpg\n2   img_91168fb0.jpg\n3   img_9822190f.jpg\n4  img_e5fc436c.jpeg","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>img_cdf80d6f.jpeg</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>img_c0142a80.jpg</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>img_91168fb0.jpg</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>img_9822190f.jpg</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>img_e5fc436c.jpeg</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":1},{"cell_type":"markdown","source":"## Prepare the data for pytorch\n\n### Subtask:\nCreate PyTorch Datasets and DataLoaders to efficiently load and preprocess the image data and labels for training.\n","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom PIL import Image\nimport os\n\n# Define image transformations\n# Training transformations include data augmentation\ntrain_transforms = transforms.Compose([\n    transforms.Resize((224, 224)),  # Resize images to 224x224 as required by ViT\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(15),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\n# Test transformations are typically just resizing and normalization\ntest_transforms = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\n# Create a mapping from soil type strings to integers\nsoil_types = train_labels_df['soil_type'].unique()\nsoil_type_to_int = {soil_type: i for i, soil_type in enumerate(soil_types)}\nint_to_soil_type = {i: soil_type for soil_type, i in soil_type_to_int.items()}\n\nclass SoilDataset(Dataset):\n    def __init__(self, image_dir, dataframe, transform=None, is_test=False):\n        self.image_dir = image_dir\n        self.dataframe = dataframe\n        self.transform = transform\n        self.is_test = is_test\n        if not self.is_test:\n            self.labels = dataframe['soil_type'].map(soil_type_to_int).tolist()\n        self.image_ids = dataframe['image_id'].tolist()\n\n\n    def __len__(self):\n        return len(self.dataframe)\n\n    def __getitem__(self, idx):\n        img_name = self.image_ids[idx]\n        img_path = os.path.join(self.image_dir, img_name)\n        image = Image.open(img_path).convert('RGB') # Ensure image is in RGB format\n\n        if self.transform:\n            image = self.transform(image)\n\n        if self.is_test:\n            return image, img_name # Return image and its name for test set\n        else:\n            label = self.labels[idx]\n            return image, label\n\n# Define the image directories\ntrain_image_dir = os.path.join('/kaggle/input/soil-classification', 'soil_classification-2025', 'train')\ntest_image_dir = os.path.join('/kaggle/input/soil-classification', 'soil_classification-2025', 'test')\n\n# Create Dataset instances\ntrain_dataset = SoilDataset(train_image_dir, train_labels_df, transform=train_transforms)\ntest_dataset = SoilDataset(test_image_dir, test_ids_df, transform=test_transforms, is_test=True)\n\n# Create DataLoader instances\nbatch_size = 32\ntrain_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\nprint(f\"Number of training samples: {len(train_dataset)}\")\nprint(f\"Number of test samples: {len(test_dataset)}\")\nprint(f\"Number of batches in train_dataloader: {len(train_dataloader)}\")\nprint(f\"Number of batches in test_dataloader: {len(test_dataloader)}\")\nprint(f\"Soil type to integer mapping: {soil_type_to_int}\")\nprint(f\"Integer to soil type mapping: {int_to_soil_type}\")\n\n# Example of getting a batch from train_dataloader\nimages, labels = next(iter(train_dataloader))\nprint(f\"\\nExample batch from train_dataloader:\")\nprint(f\"Images shape: {images.shape}\")\nprint(f\"Labels shape: {labels.shape}\")\n\n# Example of getting a batch from test_dataloader\ntest_images, test_image_names = next(iter(test_dataloader))\nprint(f\"\\nExample batch from test_dataloader:\")\nprint(f\"Images shape: {test_images.shape}\")\nprint(f\"Image names (first 5): {test_image_names[:5]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T16:00:39.483029Z","iopub.execute_input":"2025-05-24T16:00:39.483814Z","iopub.status.idle":"2025-05-24T16:00:40.907581Z","shell.execute_reply.started":"2025-05-24T16:00:39.483787Z","shell.execute_reply":"2025-05-24T16:00:40.906752Z"}},"outputs":[{"name":"stdout","text":"Number of training samples: 1222\nNumber of test samples: 341\nNumber of batches in train_dataloader: 39\nNumber of batches in test_dataloader: 11\nSoil type to integer mapping: {'Alluvial soil': 0, 'Clay soil': 1, 'Red soil': 2, 'Black Soil': 3}\nInteger to soil type mapping: {0: 'Alluvial soil', 1: 'Clay soil', 2: 'Red soil', 3: 'Black Soil'}\n\nExample batch from train_dataloader:\nImages shape: torch.Size([32, 3, 224, 224])\nLabels shape: torch.Size([32])\n\nExample batch from test_dataloader:\nImages shape: torch.Size([32, 3, 224, 224])\nImage names (first 5): ('img_cdf80d6f.jpeg', 'img_c0142a80.jpg', 'img_91168fb0.jpg', 'img_9822190f.jpg', 'img_e5fc436c.jpeg')\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"## Define the vit model\n\n### Subtask:\nImplement or load a pre-trained Vision Transformer model using PyTorch.\n","metadata":{}},{"cell_type":"code","source":"\nimport torchvision.models as models\nfrom torchvision.models import ViT_B_16_Weights # Import the specific weights class\n\n# 1. Import the vit_base_patch16_224 model and 2. Load a pre-trained version\n# Use weights=models.ViT_B_16_Weights.IMAGENET1K_V1 for pretrained weights\nmodel = models.vit_b_16(weights=ViT_B_16_Weights.IMAGENET1K_V1)\n\n# 3. Modify the classifier layer (fc) to match the number of unique soil classes\nnum_classes = len(soil_type_to_int) # Get the number of unique classes from the previously created mapping\nmodel.heads.head = torch.nn.Linear(model.heads.head.in_features, num_classes)\n\n# 4. Move the model to the appropriate device (GPU if available, otherwise CPU)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\nprint(f\"Model loaded and moved to device: {device}\")\nprint(f\"Number of output classes in the modified classifier: {model.heads.head.out_features}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Set up the training environment\n\n### Subtask:\nDefine the loss function, optimizer, and potentially a learning rate scheduler for training.\n","metadata":{}},{"cell_type":"code","source":"import torch.optim as optim\nfrom torch.optim import lr_scheduler\n\n# setting the loss to cross entropy loss for multi label calssification \ncriterion = torch.nn.CrossEntropyLoss()\n# setting the optimizer to adamW\noptimizer = optim.AdamW(model.parameters(), lr=1e-4)\n# using step learning rate scheduling \nscheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n\n\nprint(\"Loss Function:\", criterion)\nprint(\"Optimizer:\", optimizer)\nprint(\"Learning Rate Scheduler:\", scheduler)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T16:01:31.977486Z","iopub.execute_input":"2025-05-24T16:01:31.977757Z","iopub.status.idle":"2025-05-24T16:01:31.983734Z","shell.execute_reply.started":"2025-05-24T16:01:31.977734Z","shell.execute_reply":"2025-05-24T16:01:31.983107Z"}},"outputs":[{"name":"stdout","text":"Loss Function: CrossEntropyLoss()\nOptimizer: AdamW (\nParameter Group 0\n    amsgrad: False\n    betas: (0.9, 0.999)\n    capturable: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    fused: None\n    initial_lr: 0.0001\n    lr: 0.0001\n    maximize: False\n    weight_decay: 0.01\n)\nLearning Rate Scheduler: <torch.optim.lr_scheduler.StepLR object at 0x7f0ec65916d0>\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"## Train the model\n\n### Subtask:\nImplement the training loop, including forward passes, backward passes, and optimizer steps.\n","metadata":{}},{"cell_type":"code","source":"import time\n\n\nnum_epochs = 20\nprint(\"Starting training...\")\nfor epoch in range(num_epochs):\n    model.train()\n\n    running_loss = 0.0\n    start_time = time.time()\n\n\n    for images, labels in train_dataloader:\n        # send everything to gpu \n        images = images.to(device)\n        labels = labels.to(device)\n\n        # making the optimizer 0 so that the values dont stack \n        optimizer.zero_grad()\n\n        # forward pass\n        outputs = model(images)\n    \n        # calculating the loss\n        loss = criterion(outputs, labels)\n\n        # doing backpropagation \n        loss.backward()\n\n        # doing optimizer step \n        optimizer.step()\n\n        running_loss += loss.item() * images.size(0)\n\n\n    epoch_loss = running_loss / len(train_dataset)\n    epoch_time = time.time() - start_time\n\n\n    if scheduler is not None:\n        scheduler.step()\n\n\n    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Time: {epoch_time:.2f}s\")\n\nprint(\"Training finished.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T16:02:34.867048Z","iopub.execute_input":"2025-05-24T16:02:34.867341Z","iopub.status.idle":"2025-05-24T16:14:23.445341Z","shell.execute_reply.started":"2025-05-24T16:02:34.867319Z","shell.execute_reply":"2025-05-24T16:14:23.444539Z"}},"outputs":[{"name":"stdout","text":"Starting training...\nEpoch 1/20, Loss: 0.1437, Time: 34.10s\nEpoch 2/20, Loss: 0.0961, Time: 34.84s\nEpoch 3/20, Loss: 0.0803, Time: 34.11s\nEpoch 4/20, Loss: 0.0582, Time: 38.18s\nEpoch 5/20, Loss: 0.0451, Time: 39.82s\nEpoch 6/20, Loss: 0.0633, Time: 39.89s\nEpoch 7/20, Loss: 0.0312, Time: 38.01s\nEpoch 11/20, Loss: 0.0134, Time: 35.02s\nEpoch 12/20, Loss: 0.0128, Time: 33.51s\nEpoch 13/20, Loss: 0.0131, Time: 33.33s\nEpoch 14/20, Loss: 0.0105, Time: 33.78s\nEpoch 15/20, Loss: 0.0095, Time: 36.84s\nEpoch 16/20, Loss: 0.0114, Time: 35.43s\nEpoch 17/20, Loss: 0.0090, Time: 36.11s\nEpoch 18/20, Loss: 0.0103, Time: 33.46s\nEpoch 19/20, Loss: 0.0100, Time: 33.49s\nEpoch 20/20, Loss: 0.0095, Time: 33.60s\nTraining finished.\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"## Evaluate the model and generate submission file\n\n### Subtask:\nEvaluate the trained model on the test dataset and generate a submission file with predictions.","metadata":{}},{"cell_type":"code","source":"\nimport pandas as pd\n\n\n# setting the model to evaluation mode\nmodel.eval()\n\n\npredictions = []\nimage_ids = []\n\n\nwith torch.no_grad():\n\n    for images, img_names in test_dataloader:\n        # testing the model\n        images = images.to(device)\n\n        outputs = model(images)\n\n        _, predicted = torch.max(outputs.data, 1)\n\n        predicted_soil_types = [int_to_soil_type[pred.item()] for pred in predicted]\n\n        predictions.extend(predicted_soil_types)\n        image_ids.extend(img_names)\n\n\nsubmission_df = pd.DataFrame({'image_id': image_ids, 'soil_type': predictions})\n\n\nprint(\"Submission DataFrame head:\")\ndisplay(submission_df.head())\n\n# making the submission.csv file for competition \nsubmission_df.to_csv('submission.csv', index=False)\n\nprint(\"\\nSubmission file 'submission.csv' created.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T16:56:16.546240Z","iopub.execute_input":"2025-05-24T16:56:16.546527Z","iopub.status.idle":"2025-05-24T16:56:22.195874Z","shell.execute_reply.started":"2025-05-24T16:56:16.546506Z","shell.execute_reply":"2025-05-24T16:56:22.195339Z"}},"outputs":[{"name":"stdout","text":"Submission DataFrame head:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"            image_id      soil_type\n0  img_cdf80d6f.jpeg  Alluvial soil\n1   img_c0142a80.jpg  Alluvial soil\n2   img_91168fb0.jpg  Alluvial soil\n3   img_9822190f.jpg  Alluvial soil\n4  img_e5fc436c.jpeg  Alluvial soil","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_id</th>\n      <th>soil_type</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>img_cdf80d6f.jpeg</td>\n      <td>Alluvial soil</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>img_c0142a80.jpg</td>\n      <td>Alluvial soil</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>img_91168fb0.jpg</td>\n      <td>Alluvial soil</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>img_9822190f.jpg</td>\n      <td>Alluvial soil</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>img_e5fc436c.jpeg</td>\n      <td>Alluvial soil</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"name":"stdout","text":"\nSubmission file 'submission.csv' created.\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"## Calculate F1 score","metadata":{}},{"cell_type":"code","source":"\nfrom sklearn.metrics import f1_score\n\n\n\nmodel.eval()\n\ntrain_predictions = []\ntrain_true_labels = []\n\nwith torch.no_grad():\n    # calculating the f1 score on train dataset because true labels are not given of test data \n    for images, labels in train_dataloader:\n\n        images = images.to(device)\n        labels = labels.to(device)\n\n\n        outputs = model(images)\n\n        # basically doing argmax here \n        _, predicted = torch.max(outputs.data, 1)\n\n\n        train_true_labels.extend(labels.cpu().tolist())\n        train_predictions.extend(predicted.cpu().tolist())\n\n# calculating the weighted f1 score \nf1_train = f1_score(train_true_labels, train_predictions, average='weighted')\n\nprint(f\"Calculated F1 Score on Training Data: {f1_train:.4f}\")\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T16:56:59.635374Z","iopub.execute_input":"2025-05-24T16:56:59.635982Z","iopub.status.idle":"2025-05-24T16:57:18.288580Z","shell.execute_reply.started":"2025-05-24T16:56:59.635957Z","shell.execute_reply":"2025-05-24T16:57:18.287810Z"}},"outputs":[{"name":"stdout","text":"Calculated F1 Score on Training Data: 0.9975\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}